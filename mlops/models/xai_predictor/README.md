# XAI Academic Risk Predictor

An explainable AI model for predicting student academic risk using the OULAD dataset.

## ğŸ¯ Overview

This package provides:
- **Binary Classification**: Predicts if a student is at-risk (Fail/Withdrawn) or safe (Pass/Distinction)
- **Explainability**: SHAP-based explanations for individual predictions
- **86%+ Accuracy**: Trained on 32,000+ student records from Open University

## ğŸ“ Project Structure

```
xai_predictor/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ pyproject.toml               # Project configuration
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py              # Centralized configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ etl.py               # ETL pipeline
â”‚   â”‚   â””â”€â”€ preprocessing.py     # Feature engineering
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py           # Model training
â”‚   â”‚   â”œâ”€â”€ predictor.py         # Inference
â”‚   â”‚   â””â”€â”€ explainer.py         # SHAP explanations
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ metrics.py           # Custom metrics
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_etl.py
â”‚   â”œâ”€â”€ test_trainer.py
â”‚   â””â”€â”€ test_predictor.py
â”œâ”€â”€ notebooks/                   # Jupyter experiments
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ data/                        # Data files (git-ignored)
â”‚   â”œâ”€â”€ raw/                     # Original OULAD data
â”‚   â””â”€â”€ processed/               # Transformed data
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ models/                  # Trained model files
â”‚   â””â”€â”€ metrics/                 # Evaluation results
â””â”€â”€ scripts/
    â””â”€â”€ run_pipeline.py          # CLI entrypoint
```

## ğŸš€ Quick Start

### Installation

```bash
cd ml/models/xai_predictor
pip install -e .
```

### Training

```bash
# Full pipeline (ETL + Train + Validate)
python -m scripts.run_pipeline

# Individual steps
python -m scripts.run_pipeline --step etl
python -m scripts.run_pipeline --step train
python -m scripts.run_pipeline --step validate
```

### Prediction

```python
from src.models.predictor import RiskPredictor

predictor = RiskPredictor()

# Single prediction
result = predictor.predict({
    "avg_grade": 65.0,
    "grade_consistency": 85.0,
    "num_assessments": 5,
    "assessment_completion_rate": 0.8,
    "studied_credits": 60,
    "num_of_prev_attempts": 0,
})

print(f"Risk Level: {result['risk_level']}")
print(f"Probability: {result['probability']:.2%}")
```

### Explanations

```python
from src.models.explainer import RiskExplainer

explainer = RiskExplainer()
explanation = explainer.explain(student_features)

print(explanation['top_factors'])
# [('assessment_completion_rate', -0.15), ('avg_grade', 0.08), ...]
```

## ğŸ“Š Model Details

| Metric | Value |
|--------|-------|
| Algorithm | XGBoost Classifier |
| Accuracy | 86.2% |
| Precision | 91.6% |
| Recall | 81.3% |
| F1 Score | 86.2% |

### Features (10)

| Feature | Description |
|---------|-------------|
| `avg_grade` | Average assessment score (0-100) |
| `grade_consistency` | Performance consistency (100 - std) |
| `grade_range` | Score variability (max - min) |
| `num_assessments` | Number of assessments completed |
| `assessment_completion_rate` | Completion rate (0-1) |
| `studied_credits` | Course credits enrolled |
| `num_of_prev_attempts` | Number of previous attempts |
| `low_performance` | Binary: grade < 40% |
| `low_engagement` | Binary: low assessment completion |
| `has_previous_attempts` | Binary: has failed before |

## ğŸ”¬ Explainability

The model uses SHAP (SHapley Additive exPlanations) to provide:

1. **Feature Importance**: Which features matter most globally
2. **Individual Explanations**: Why a specific student was classified
3. **Counterfactual Analysis**: What changes would alter the prediction

## ğŸ“ˆ Data Source

[OULAD - Open University Learning Analytics Dataset](https://analyse.kmi.open.ac.uk/open_dataset)

- 32,593 student records
- 7 courses over 2 years
- Assessment and VLE interaction data

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ”„ Migration Guide

If you're upgrading from the legacy flat structure:

### Update Imports

```python
# Old (legacy)
from train import train_model
import xgboost as xgb
model = xgb.XGBClassifier()
model.load_model("saved_models/model.json")

# New (restructured)
from src.models.trainer import Trainer
from src.models.predictor import RiskPredictor

predictor = RiskPredictor()  # Handles model loading automatically
```

### Update CLI Commands

```bash
# Old
python train.py --step all

# New  
python -m scripts.run_pipeline --step all
```

### Legacy File Compatibility

The `saved_models/` directory is still supported for backward compatibility.
New models will be saved there unless configured otherwise in `config/settings.py`.

## ğŸ“ License

MIT License - EduMind Team
